{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e011c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/homebrew/Caskroom/miniconda/base/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /Users/gui_r/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/gui_r/.ivy2/jars\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-292d4a96-f6ff-4976-beac-2062eeaa255e;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/homebrew/Caskroom/miniconda/base/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 255ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-292d4a96-f6ff-4976-beac-2062eeaa255e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/4ms)\n",
      "22/05/06 12:59:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/06 12:59:18 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/05/06 12:59:18 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\n",
      "+-----------------+-----+--------------------+--------------------+----------+\n",
      "|         features|label|       rawPrediction|         probability|prediction|\n",
      "+-----------------+-----+--------------------+--------------------+----------+\n",
      "|[4.3,3.0,1.1,0.1]|  0.0|[6.54087870810077...|[0.98343057765760...|       0.0|\n",
      "|[4.6,3.2,1.4,0.2]|  0.0|[6.11478298858913...|[0.97829709218680...|       0.0|\n",
      "|[4.6,3.4,1.4,0.3]|  0.0|[6.35512818379031...|[0.98714780682076...|       0.0|\n",
      "|[4.7,3.2,1.6,0.2]|  0.0|[5.79309209783107...|[0.96929858420403...|       0.0|\n",
      "|[4.8,3.0,1.4,0.1]|  0.0|[5.62923168862030...|[0.94834890280808...|       0.0|\n",
      "|[4.9,2.5,4.5,1.7]|  2.0|[-2.1028499239658...|[0.02048751526399...|       1.0|\n",
      "|[4.9,3.0,1.4,0.2]|  0.0|[5.29458787529601...|[0.93189909625800...|       0.0|\n",
      "|[4.9,3.1,1.5,0.1]|  0.0|[5.63327769512019...|[0.95087485767876...|       0.0|\n",
      "|[4.9,3.1,1.5,0.1]|  0.0|[5.63327769512019...|[0.95087485767876...|       0.0|\n",
      "|[5.0,3.2,1.2,0.2]|  0.0|[5.82345861742808...|[0.96272448708194...|       0.0|\n",
      "+-----------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Accuracy 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "!pip install kafka-python\n",
    "!pip install pyspark\n",
    "!curl -sSOL https://dlcdn.apache.org/kafka/3.1.0/kafka_2.13-3.1.0.tgz\n",
    "!tar -xzf kafka_2.13-3.1.0.tgz\n",
    "!./kafka_2.13-3.1.0/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-3.1.0/config/zookeeper.properties\n",
    "!./kafka_2.13-3.1.0/bin/kafka-server-start.sh -daemon ./kafka_2.13-3.1.0/config/server.properties\n",
    "!sleep 10\n",
    "!./kafka_2.13-3.1.0/bin/kafka-topics.sh --delete --topic input_topic --bootstrap-server localhost:9092\n",
    "!./kafka_2.13-3.1.0/bin/kafka-topics.sh --create --topic input_topic --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1\n",
    "!./kafka_2.13-3.1.0/bin/kafka-topics.sh --create --topic output_topic --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1\n",
    "!./kafka_2.13-3.1.0/bin/kafka-console-consumer.sh  --bootstrap-server localhost:9092 --topic input_topic --from-beginning\n",
    "!rm iris.csv\n",
    "!wget https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv\n",
    "!ls\n",
    "'''\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import threading\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "from kafka.errors import KafkaError\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "from json import dumps\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "conf = pyspark.SparkConf().set('spark.jars.packages', 'org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0')\n",
    "sc = pyspark.SparkContext(conf = conf)\n",
    "ssc = StreamingContext(sc,1)\n",
    "KAFKA_INPUT = \"input_topic\"\n",
    "KAFKA_OUTPUT = \"output_topic\"\n",
    "KAFKA_SERVER = \"127.0.0.1:9092\"\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\",\",\").load(\"iris.csv\")\n",
    "original_schema = data.schema\n",
    "feature_cols = data.columns[:-1]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "data = data.select(*(col(c).cast(\"double\") for c in feature_cols), \"species\")\n",
    "data = assembler.transform(data)\n",
    "data = data.select(['features', 'species'])\n",
    "label_indexer = StringIndexer(inputCol='species', outputCol='label').fit(data)\n",
    "data = label_indexer.transform(data)\n",
    "data = data.select(['features', 'label'])\n",
    "\n",
    "reg = 0.01\n",
    "train, test = data.randomSplit([0.70, 0.30])\n",
    "lr = LogisticRegression(regParam=reg)\n",
    "model = lr.fit(train)\n",
    "prediction = model.transform(test)\n",
    "\n",
    "print(\"Prediction\")\n",
    "prediction.show(10)\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "\n",
    "print('Accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6809bb47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stream_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df2 \u001b[38;5;241m=\u001b[39m \u001b[43mstream_df\u001b[49m\u001b[38;5;241m.\u001b[39mselect( from_json(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m), original_schema)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues.*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stream_df' is not defined"
     ]
    }
   ],
   "source": [
    "df2 = stream_df.select( from_json(col(\"value\").cast(\"string\"), original_schema).alias(\"values\")).select(\"values.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33ea25da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(\"Prediction\")\\nprediction.show(10)\\n\\nevaluator = MulticlassClassificationEvaluator(metricName=\\'accuracy\\')\\naccuracy = evaluator.evaluate(prediction)\\nprint(\\'Accuracy\\', accuracy)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_df = spark \\\n",
    "            .readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "            .option(\"subscribe\", KAFKA_INPUT) \\\n",
    "            .option(\"startingOffsets\", \"latest\") \\\n",
    "            .load()\n",
    "\n",
    "'''query = (stream_df.writeStream\n",
    "                  .outputMode(\"append\")\n",
    "                  .format(\"memory\")\n",
    "                  .queryName(\"kafka\")\n",
    "                  .start())'''\n",
    "\n",
    "\n",
    "#df = spark.table(\"kafka\")\n",
    "#df2 = df.select( from_json(col(\"value\").cast(\"string\"), original_schema).alias(\"values\")).select(\"values.*\")\n",
    "df2 = stream_df.select( from_json(col(\"value\").cast(\"string\"), original_schema).alias(\"values\")).select(\"values.*\")\n",
    "\n",
    "data = df2\n",
    "feature_cols = data.columns[:-1]\n",
    "data = data.select(*(col(c).cast(\"double\") for c in feature_cols), \"species\")\n",
    "data = assembler.transform(data)\n",
    "data = data.select(['features', 'species'])\n",
    "data = label_indexer.transform(data)\n",
    "data = data.select(['features', 'label'])\n",
    "\n",
    "prediction = model.transform(data)\n",
    "\n",
    "'''print(\"Prediction\")\n",
    "prediction.show(10)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print('Accuracy', accuracy)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24786d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.select(\"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc16782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/06 12:59:48 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "prediction \\\n",
    "  .select(to_json(struct([col(c).alias(c) for c in prediction.columns])).alias(\"value\"))\\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "  .option(\"topic\", KAFKA_OUTPUT) \\\n",
    "  .start() \\\n",
    "  .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b235093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
